
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Logistic Regression | Pumpkin</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Logistic Regression (以下简称LR)解决的是分类问题. 考虑两类分类问题, 如判断邮件是否是垃圾邮件, 或肿瘤是否为恶性肿瘤, 样本标签只有两类: 0表示negtive, 1表示positive. 考虑用之前讨论过的Linear regression来解决这个问题, 如图1所示, 当样本分布理想时, 可以通过设定阈值来对样本进行分类 (绿色); 但是当样本分布不理想时, 拟合出">
<meta property="og:type" content="article">
<meta property="og:title" content="Logistic Regression">
<meta property="og:url" content="http://bearpaw.github.io/blog/blog/2015/02/09/Logistic-Regression/">
<meta property="og:site_name" content="Pumpkin">
<meta property="og:description" content="Logistic Regression (以下简称LR)解决的是分类问题. 考虑两类分类问题, 如判断邮件是否是垃圾邮件, 或肿瘤是否为恶性肿瘤, 样本标签只有两类: 0表示negtive, 1表示positive. 考虑用之前讨论过的Linear regression来解决这个问题, 如图1所示, 当样本分布理想时, 可以通过设定阈值来对样本进行分类 (绿色); 但是当样本分布不理想时, 拟合出">
<meta property="og:image" content="http://7u2pi6.com1.z0.glb.clouddn.com/blog/linear-regression-for-classification.png">
<meta property="og:image" content="http://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg">
<meta property="og:image" content="http://7u2pi6.com1.z0.glb.clouddn.com/blog/logistic-regression-loss.jpg">
<meta property="og:image" content="http://7u2pi6.com1.z0.glb.clouddn.com/blog/sigmoid-gradient.png">
<meta property="og:image" content="http://7u2pi6.com1.z0.glb.clouddn.com/blog/logistic-regression-gradient.png">
<meta property="og:image" content="http://7u2pi6.com1.z0.glb.clouddn.com/blog/newton-method-1d.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Logistic Regression">
<meta name="twitter:description" content="Logistic Regression (以下简称LR)解决的是分类问题. 考虑两类分类问题, 如判断邮件是否是垃圾邮件, 或肿瘤是否为恶性肿瘤, 样本标签只有两类: 0表示negtive, 1表示positive. 考虑用之前讨论过的Linear regression来解决这个问题, 如图1所示, 当样本分布理想时, 可以通过设定阈值来对样本进行分类 (绿色); 但是当样本分布不理想时, 拟合出">
<link rel="publisher" href="https://plus.google.com/u/0/+WeiYangplatero">

  
    <link rel="alternative" href="/blog/atom.xml" title="Pumpkin" type="application/atom+xml">
  
  
    <link rel="icon" href="/blog/favicon.ico">
  
  <link rel="stylesheet" href="/blog/css/style.css" type="text/css">

  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  
<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v1/st.js','_st');

  _st('install','C3e3fw-XzPj6xPmhGvHa');
</script>
</head>
<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/blog/" id="logo">Pumpkin</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/blog/" id="subtitle">不积跬步，无以至千里 / 不积小流，无以成江海</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/blog/">Blog</a>
        
          <a class="main-nav-link" href="/blog/PRML">PRML</a>
        
          <a class="main-nav-link" href="/blog/Deep-Learning">Deep Learning</a>
        
          <a class="main-nav-link" href="/blog/archives">Archives</a>
        
          <a class="main-nav-link" href="http://bearpaw.github.io">About</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/blog/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      
        <div id="search-form-wrap">
          <form class="search-form" action="/blog/search/index.html" method="get" accept-charset="utf-8">
              <input type="text" id="st-search-input" class="search-form-input" placeholder="Search" />
              <input type="submit" value="" class="search-form-submit"> 
           </form>
        </div>
      
    </div>
  </div>
</header>
    <div class="outer">
      <section id="main"><article id="post-Logistic-Regression" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/blog/2015/02/09/Logistic-Regression/" class="article-date">
  <time datetime="2015-02-09T01:26:01.000Z" itemprop="datePublished">2月 9 2015</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/blog/categories/知识库/">知识库</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Logistic Regression
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      	  
		<!-- 文章目录开始 -->
		
			<div id="toc" class="toc-article">
				<strong class="toc-title">文章目录</strong>
			<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#模型"><span class="toc-number">1.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#损失函数"><span class="toc-number">2.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参数估计"><span class="toc-number">3.</span> <span class="toc-text">参数估计</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度下降法"><span class="toc-number">3.1.</span> <span class="toc-text">梯度下降法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#牛顿法"><span class="toc-number">3.2.</span> <span class="toc-text">牛顿法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度下降法_vs_牛顿法"><span class="toc-number">3.3.</span> <span class="toc-text">梯度下降法 vs 牛顿法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#推广:_Softmax_Regression"><span class="toc-number">4.</span> <span class="toc-text">推广: Softmax Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考资料"><span class="toc-number">5.</span> <span class="toc-text">参考资料</span></a></li></ol>
			</div>
		
		<!-- 文章目录结束 -->
        <p>Logistic Regression (以下简称LR)解决的是分类问题. 考虑两类分类问题, 如判断邮件是否是垃圾邮件, 或肿瘤是否为恶性肿瘤, 样本标签只有两类: 0表示negtive, 1表示positive. 考虑用之前讨论过的Linear regression来解决这个问题, 如图1所示, 当样本分布理想时, 可以通过设定阈值来对样本进行分类 (绿色); 但是当样本分布不理想时, 拟合出来的linear function偏差比较大, 设定阈值的方法不能获得理想的分类结果 (蓝色). Linear regression的另一个问题是其给出的预测范围是 $h(x) \in (-\infty, \infty)$, 而分类问题的标签就两个$\{0, 1\}$, 其理想的predict function应该是 $h(x) \in [0, 1]$, 所以linear regression也不适合分类问题.</p>
<p><img src="http://7u2pi6.com1.z0.glb.clouddn.com/blog/linear-regression-for-classification.png" alt="图1. 用linear regression模型解决分类问题"></p>
<p><a id="more"></a></p>
<h2 id="模型">模型</h2>
<p><img src="http://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg" alt="图2. Sigmoid function (Logistic function)"></p>
<p>回忆linear regression的模型为 <span>$h_\theta(x) = \theta^T x \in  (-\infty, \infty)$</span>, 然而前面已经分析过, 理想的模型是 <span>$h_\theta(x) \in [0, 1]$</span>. 马上能想到的就是sigmoid function (又称logistic function), 其函数形式为 $g(z) = \frac{1}{1 + \exp^{-z}}$, 如图2所示. 因此, 我们的分类模型可以定义如下</p>
<span>$$h_\theta(x) = g(\theta^T x) = \frac{1}{1+\exp^{-\theta^T x}} \in [0, 1].$$</span>

<p>因为用到了logistic function, 所以这种模型就被称为logistic regression.</p>
<p>怎么理解这个模型呢? <span>$h_\theta(x)$</span>可以理解为给定$x$时, 估计$y=1$的概率, 即<span>$h_\theta(x) = P(y=1 | x; \theta)$</span>.</p>
<h2 id="损失函数">损失函数</h2>
<p>给定误差评价: <span>$L(h_\theta(x^{(i)}), y^{(i)})$</span>, 整个模型的损失函数可以定义如下:</p>
<span>$$J(\theta) = \frac{1}{m} \sum_{i=1}^m  L(h_\theta(x^{(i)}), y^{(i)}).$$</span><br>在linear regression里, 我们用的是squred error <span>$L(h_\theta(x^{(i)}), y^{(i)}) = (h_\theta(x^{(i)})- y^{(i)})^2$</span>来定义损失函数. 值得注意的是, $J(\theta)$是关于$\theta$的non-convex函数, 有许多local minimum. 能否找到一个convex的损失函数呢?<br><br>不考虑上标, 我们可以借助对数函数来定义loss function:<br><span>$$L(h_\theta(x), y) = 
\begin{cases} -\log h_\theta(x) & \text{if } y=1 \\ 
-\log(1 - h_\theta(x)) & \text{if } y = 0\end{cases}.$$</span>

<p>如图3所示, 我们分两种情况来分析这个loss function.</p>
<ol>
<li><strong>当$y=1$时</strong>: 若 <span>$h_\theta(x) \rightarrow 1$</span>, 则 <span>$L(h_\theta(x), y) \rightarrow 0$</span>;  若 <span>$h_\theta(x) \rightarrow 0$</span>, 则 <span>$L(h_\theta(x), y) \rightarrow \infty$</span>;</li>
<li><strong>当$y=0$时</strong>: 若 <span>$h_\theta(x) \rightarrow 0$</span>, 则 <span>$L(h_\theta(x), y) \rightarrow 0$</span>;  若 <span>$h_\theta(x) \rightarrow 1$</span>, 则 <span>$L(h_\theta(x), y) \rightarrow \infty$</span>;</li>
</ol>
<p><img src="http://7u2pi6.com1.z0.glb.clouddn.com/blog/logistic-regression-loss.jpg" alt="图3. Logistic regression的损失函数, 左边为$y=1$的情况, 右边为$y=0$的情况."></p>
<p>把$L$写成更compact的形式, 就是</p>
<p><span>$$L(h_\theta(x), y) = - y\log h_\theta(x) - (1-y)\log(1 - h_\theta(x)).$$</span><br>所以, 总的代价函数可以写成:</p>
<p><span>$$J(\theta) = \frac{1}{m} \sum_{i=1}^m L(h_\theta(x^{(i)}), y^{(i)}) = -\frac{1}{m} \sum_{i=1}^m \left(y^{(i)}\log h_\theta(x^{(i)}) + (1-y^{(i)})\log(1 - h_\theta(x^{(i)}))\right).$$</span><br>最优的参数为 <span>$\theta^* = \min_\theta J(\theta)$</span>. </p>
<p>为什么选择$\log$函数而不是其他函数呢? 毕竟很多选择都能得到图3的curve. 一个<a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="external">从统计上的推导</a>可以知道, 这是和<em>最大似然估计</em>等价的.</p>
<h2 id="参数估计">参数估计</h2>
<h3 id="梯度下降法">梯度下降法</h3>
<p>梯度下降法的说明参见<a href="2015/02/06/Linear-Regression/">Linear regression</a>. 这里关键在于求导<span>$\frac{\partial J(\theta)}{\partial \theta}$</span>. 首先, sigmoid函数求导推导如下:<br><img src="http://7u2pi6.com1.z0.glb.clouddn.com/blog/sigmoid-gradient.png" alt="sigmoid求导"><br>基于此, 只考虑一个训练样本$(x,y)$, 则代价函数的求导过程如下 (下图来自参考资料2, 是以最大似然估计来推导的, 所以差一个负号):<br><img src="http://7u2pi6.com1.z0.glb.clouddn.com/blog/logistic-regression-gradient.png" alt="Logistic regression loss function求导"><br>所以有, </p>
<p><span>$$\frac{\partial J(\theta)}{\partial \theta} = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x^{(i)}.$$</span><br>这样, 就能带入梯度下降法的更新公式里来迭代优化参数$\theta$.</p>
<h3 id="牛顿法">牛顿法</h3>
<p>当目标函数二阶可导时, 牛顿法是更快的优化算法. 我们先讨论该方法的intuition.</p>
<p>我们的目标是求解最小化目标函数的参数 <span>$\theta^* = \min_\theta J(\theta)$</span>, 我们知道理想情况下应该有 <span>$\frac{\partial J(\theta^*)}{\partial \theta^*} = 0$</span>,  那么考虑1d的情况, 即$\theta \in R$, 令 $f(\theta) = \frac{dJ(\theta)}{d \theta}$, 此时一个好的参数估计 $\theta$应该满足 $f(\theta) \rightarrow 0$. 如图4所示, 给定$\theta^t$, 那么我们求点$(\theta^t, f(\theta^t))$的切线, 并把该切线与横轴的交点作为$\theta^{t+1}$. </p>
<p><img src="http://7u2pi6.com1.z0.glb.clouddn.com/blog/newton-method-1d.png" alt="图4. 1维牛顿法示意图"></p>
<p>用数学公式可以表示为 $\theta^{t+1} = \theta^t - \Delta$, 而从梯度定义我们知道 <span>$f'(\theta) = \frac{f(\theta)}{\Delta}$</span>, 因此有<span>$\Delta= \frac{f(\theta)}{f'(\theta)}$</span>, 所以更新公式为:</p>
<p><span>$$\theta^{t+1} = \theta^t - \frac{f(\theta^t)}{f'(\theta^t)} = \theta^t - \frac{J'(\theta^t)}{J''(\theta^t)} $$</span><br>当$\theta \in R^{n+1}$时, 迭代公式为:</p>
<p><span>$$\theta^{t+1} = \theta^t - H^{-1}\nabla J(\theta^t).$$</span><br>其中$H$是二阶导矩阵, 即Hessian matrix.</p>
<h3 id="梯度下降法_vs_牛顿法">梯度下降法 vs 牛顿法</h3>
<p>何时使用梯度下降法(G), 何时使用牛顿法(N)呢?</p>
<ol>
<li>G更容易实现, 可以作为算法开发初期的starter method.</li>
<li>G需要调参($\alpha$), 而N没有参数.</li>
<li>G收敛比N慢, 但是N更容易陷入局部最优, 因此在实践中往往先用G找到一个不错的initial solution, 再用N从这个initial solution出发进行优化(<a href="http://www.zhihu.com/question/19723347" target="_blank" rel="external">参考知乎</a>)</li>
<li>虽然G收敛需要较多迭代, 但是每次迭代的计算复杂度低(O(n), 其中n是数据维度); 而牛顿法每一次迭代都需要计算Hessian matrix的逆, 计算复杂度高 ($O(n^3)$). 因此当数据维度小时可以用牛顿法 (n&lt;=1000), 当数据维度高时用梯度下降法.</li>
</ol>
<h2 id="推广:_Softmax_Regression">推广: Softmax Regression</h2>
<p>请参考<a href="http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92" target="_blank" rel="external">UFLDL教程</a>.</p>
<h2 id="参考资料">参考资料</h2>
<ol>
<li><a href="http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=DeepLearning" target="_blank" rel="external">OpenClassroom-Deep Learning</a>, Samy Bengio, Tom Dean and Andrew Ng</li>
<li><a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="external">Stanford CS229 Lecture notes.</a> Andrew Ng</li>
<li><a href="http://www.zhihu.com/question/19723347" target="_blank" rel="external">知乎: 最优化问题中，牛顿法为什么比梯度下降法求解需要的迭代次数更少？</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/Logistic_Regression_Vectorization_Example" target="_blank" rel="external">Logistic Regression Vectorization Example</a>.UFLDL. </li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92" target="_blank" rel="external">Softmax回归</a>. UFLDL. </li>
</ol>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://bearpaw.github.io/blog/2015/02/09/Logistic-Regression/" data-id="efjj2ytw7r1imgbl" class="article-share-link" data-share="baidu">分享到</a>
      

      
        <a href="http://bearpaw.github.io/blog/2015/02/09/Logistic-Regression/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Logistic-regression/">Logistic regression</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/classification/">classification</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/gradient-descent/">gradient descent</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/newton-method/">newton method</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/optimization/">optimization</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/blog/2015/03/19/Articulated-Pose-Estimation-by-a-Graphical-Model-with-Image-Dependent-Pairwise-Relations/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Articulated Pose Estimation by a Graphical Model with Image Dependent Pairwise Relations
        
      </div>
    </a>
  
  
    <a href="/blog/2015/02/06/Linear-Regression/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Linear Regression</div>
    </a>
  
</nav>

  
</article>


  <section id="comments">
    <div id="ds-thread" class="ds-thread" data-thread-key="2015/02/09/Logistic-Regression/" data-title="Logistic Regression" data-url="http://bearpaw.github.io/blog/2015/02/09/Logistic-Regression/"></div>
  </section>
</section>
      
      <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/blog/categories/工具/">工具</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/机器学习/">机器学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/知识库/">知识库</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/论文笔记/">论文笔记</a><span class="category-list-count">5</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/Caffe/">Caffe</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/DPM/">DPM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/Deep-Learning/">Deep Learning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/Deformation-Layer/">Deformation Layer</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/HOG/">HOG</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/Logistic-regression/">Logistic regression</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/MCMC/">MCMC</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/Pedestrian-Detection/">Pedestrian Detection</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/articulated-pose-estimation/">articulated pose estimation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/classification/">classification</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/distance-transform/">distance transform</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/feature/">feature</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/git/">git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/gradient-descent/">gradient descent</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/hinge-loss/">hinge loss</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/human-detection/">human detection</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/latent-ssvm/">latent ssvm</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/linear-regression/">linear regression</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/mixtures-of-parts/">mixtures of parts</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/newton-method/">newton method</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/optimization/">optimization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/pictorial-model/">pictorial model</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/pose-estimation/">pose estimation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/sampling/">sampling</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/ssvm/">ssvm</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/blog/tags/Caffe/" style="font-size: 10.00px;">Caffe</a><a href="/blog/tags/DPM/" style="font-size: 10.00px;">DPM</a><a href="/blog/tags/Deep-Learning/" style="font-size: 10.00px;">Deep Learning</a><a href="/blog/tags/Deformation-Layer/" style="font-size: 10.00px;">Deformation Layer</a><a href="/blog/tags/HOG/" style="font-size: 20.00px;">HOG</a><a href="/blog/tags/Logistic-regression/" style="font-size: 10.00px;">Logistic regression</a><a href="/blog/tags/MCMC/" style="font-size: 10.00px;">MCMC</a><a href="/blog/tags/Pedestrian-Detection/" style="font-size: 10.00px;">Pedestrian Detection</a><a href="/blog/tags/articulated-pose-estimation/" style="font-size: 10.00px;">articulated pose estimation</a><a href="/blog/tags/classification/" style="font-size: 10.00px;">classification</a><a href="/blog/tags/distance-transform/" style="font-size: 10.00px;">distance transform</a><a href="/blog/tags/feature/" style="font-size: 10.00px;">feature</a><a href="/blog/tags/git/" style="font-size: 10.00px;">git</a><a href="/blog/tags/gradient-descent/" style="font-size: 20.00px;">gradient descent</a><a href="/blog/tags/hinge-loss/" style="font-size: 10.00px;">hinge loss</a><a href="/blog/tags/human-detection/" style="font-size: 10.00px;">human detection</a><a href="/blog/tags/latent-ssvm/" style="font-size: 10.00px;">latent ssvm</a><a href="/blog/tags/linear-regression/" style="font-size: 10.00px;">linear regression</a><a href="/blog/tags/mixtures-of-parts/" style="font-size: 10.00px;">mixtures of parts</a><a href="/blog/tags/newton-method/" style="font-size: 10.00px;">newton method</a><a href="/blog/tags/optimization/" style="font-size: 10.00px;">optimization</a><a href="/blog/tags/pictorial-model/" style="font-size: 10.00px;">pictorial model</a><a href="/blog/tags/pose-estimation/" style="font-size: 10.00px;">pose estimation</a><a href="/blog/tags/sampling/" style="font-size: 10.00px;">sampling</a><a href="/blog/tags/ssvm/" style="font-size: 10.00px;">ssvm</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2015/04/">April 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2015/03/">March 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2015/02/">February 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2015/01/">January 2015</a><span class="archive-list-count">4</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">近期文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/blog/2015/04/05/Joint-Deep-Learning-for-Pedestrian-Detection/">Joint Deep Learning for Pedestrian Detection</a>
          </li>
        
          <li>
            <a href="/blog/2015/04/03/latent-structural-svm/">Latent Structural SVM</a>
          </li>
        
          <li>
            <a href="/blog/2015/04/01/Generalized-Distance-Transform/">Generalized Distance Transform</a>
          </li>
        
          <li>
            <a href="/blog/2015/03/25/Git-in-a-Nutshell/">Git in a Nutshell</a>
          </li>
        
          <li>
            <a href="/blog/2015/03/19/Articulated-Pose-Estimation-by-a-Graphical-Model-with-Image-Dependent-Pairwise-Relations/">Articulated Pose Estimation by a Graphical Model with Image Dependent Pairwise Relations</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">友情链接</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="http://mmlab.ie.cuhk.edu.hk/" target="_blank">CUHK MMLAB</a>
          </li>
        
          <li>
            <a href="http://vision.sysu.edu.cn/" target="_blank">SYSU IMC LAB</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 Plateo<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/xiangming/landscape-plus" target="_blank">Landscape-plus</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/blog/" class="mobile-nav-link">Blog</a>
  
    <a href="/blog/PRML" class="mobile-nav-link">PRML</a>
  
    <a href="/blog/Deep-Learning" class="mobile-nav-link">Deep Learning</a>
  
    <a href="/blog/archives" class="mobile-nav-link">Archives</a>
  
    <a href="http://bearpaw.github.io" class="mobile-nav-link">About</a>
  
</nav>
  <!-- totop start -->
<div id="totop">
<a title="返回顶部"><img src="/blog/img/scrollup.png"/></a>
</div>

<!-- totop end -->

<!-- 多说公共js代码 start -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"platero"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共js代码 end -->


<!-- 百度分享 start -->

<div id="article-share-box" class="article-share-box">
  <div id="bdshare" class="bdsharebuttonbox article-share-links">
    <a class="article-share-weibo" data-cmd="tsina" title="分享到新浪微博"></a>
    <a class="article-share-weixin" data-cmd="weixin" title="分享到微信"></a>
    <a class="article-share-qq" data-cmd="sqq" title="分享到QQ"></a>
    <a class="article-share-renren" data-cmd="renren" title="分享到人人网"></a>
    <a class="article-share-more" data-cmd="more" title="更多"></a>
  </div>
</div>
<script>window._bd_share_config={"common":{},"share":{"bdCustomStyle":"nocss.css"}};with(document)0[(getElementsByTagName("head")[0]||body).appendChild(createElement("script")).src="http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion="+~(-new Date()/36e5)];</script>

<!-- 百度分享 end -->

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>




<! -- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],displayMath: [['\\[','\\]'], ['$$','$$']],
                processEscapes: true
                    
}
  
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
                  
}
    
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
            for(i=0; i < all.length; i += 1) {
                            all[i].SourceElement().parentNode.className += ' has-jax';
                                    
            }
                
        });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<script src="/blog/js/script.js" type="text/javascript"></script>


</div>
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>
